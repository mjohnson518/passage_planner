# Post-Deployment Testing Strategy

**Date:** October 24, 2025  
**Context:** Backend deployed to production, orchestrator currently down (being fixed)  
**Reality Check:** Testing strategy must adapt to production reality

---

## ğŸ¯ CORE PRINCIPLE

**Fix Production Issues First, Add Tests After**

Traditional TDD says "write tests first." Production reality says "fix critical bugs immediately, then add regression tests."

**New Priority:**
1. ğŸ”´ Fix P0 critical production issues (orchestrator down)
2. ğŸŸ  Fix P1 high-priority issues (build time, frontend config)
3. ğŸŸ¡ Add regression tests for fixed bugs
4. ğŸ”µ Test based on actual user behavior patterns
5. âšª Write tests for unused features (lowest priority)

---

## ğŸš¨ PRODUCTION-FIRST TESTING APPROACH

### Phase 1: Fix Critical Issues (This Week)

**DO:**
- âœ… Fix orchestrator deployment immediately
- âœ… Verify fixes work in production
- âœ… Add smoke tests for critical paths
- âœ… Monitor production logs for errors

**DON'T:**
- âŒ Write comprehensive test suites before fixing
- âŒ Aim for 90% coverage on broken code
- âŒ Test features that don't work yet
- âŒ Perfect is enemy of good

**Testing After Fixes:**
```typescript
// Add simple smoke test after orchestrator fix
describe('Orchestrator Production', () => {
  it('should start successfully', async () => {
    const response = await fetch('https://...railway.app/health');
    expect(response.status).toBe(200);
  });
  
  it('should process passage planning requests', async () => {
    const response = await fetch('https://...railway.app/api/passage-planning/analyze', {
      method: 'POST',
      body: JSON.stringify(mockPassageRequest)
    });
    expect(response.status).toBe(200);
    const data = await response.json();
    expect(data.route).toBeDefined();
  });
});
```

**Time investment:** 30 minutes after fix complete

---

### Phase 2: Production Monitoring (Week 5)

**DO:**
- âœ… Set up error tracking (Sentry)
- âœ… Add performance monitoring (New Relic/Datadog)
- âœ… Set up uptime monitoring (UptimeRobot)
- âœ… Create alerts for critical failures
- âœ… Track user behavior (Plausible/Fathom)

**Monitoring Setup Priority:**

**1. Error Tracking (2 hours)**
```bash
npm install @sentry/node --workspace=@passage-planner/orchestrator
```

```typescript
import * as Sentry from '@sentry/node';

Sentry.init({
  dsn: process.env.SENTRY_DSN,
  environment: process.env.NODE_ENV,
  tracesSampleRate: 0.1, // 10% of transactions
});

// Add to Express error handler
app.use(Sentry.Handlers.errorHandler());
```

**Benefits:**
- Automatic error capture
- Stack traces with context
- User impact tracking
- Performance insights

**2. Uptime Monitoring (30 minutes)**
- Sign up for UptimeRobot (free)
- Monitor /health endpoint every 5 minutes
- Alert to email on downtime
- Track uptime percentage

**3. Performance Monitoring (2 hours)**
- Add response time tracking
- Monitor database query times
- Track API endpoint performance
- Set up alerts for slow responses (>3 seconds)

**4. User Analytics (1 hour)**
- Add Plausible or Fathom (privacy-friendly)
- Track page views
- Track passage creation events
- Track feature usage
- NO user tracking, just aggregate stats

---

### Phase 3: Regression Testing (Ongoing)

**Every Bug Fixed = New Test Added**

**Process:**
1. Bug reported or discovered
2. Fix bug in production immediately
3. Add regression test to prevent recurrence
4. Deploy test with next code change

**Example:**

Bug found: "Orchestrator crashes when vessel draft is missing"

Fix:
```typescript
// orchestrator/src/routes/passage-planning.ts
if (!vessel.draft || vessel.draft <= 0) {
  throw new Error('Vessel draft must be a positive number');
}
```

Regression test:
```typescript
// orchestrator/src/__tests__/passage-planning.test.ts
describe('Passage Planning Validation', () => {
  it('should reject requests without vessel draft', async () => {
    const response = await request(app)
      .post('/api/passage-planning/analyze')
      .send({
        departure: mockDeparture,
        destination: mockDestination,
        vessel: { name: 'Test', type: 'sailboat' } // Missing draft
      });
    
    expect(response.status).toBe(400);
    expect(response.body.error).toContain('draft');
  });
});
```

**Time investment:** 15-30 minutes per bug fix

---

### Phase 4: User-Behavior-Driven Testing (Weeks 6-8)

**Test What Users Actually Use**

**Approach:**
1. Monitor production analytics for 2 weeks
2. Identify most-used features
3. Identify features that break most often
4. Prioritize testing based on actual usage

**Example Insights:**

**Analytics show:**
- 80% of passages are coastal (< 50nm)
- 15% are offshore (50-200nm)
- 5% are ocean crossing (> 200nm)
- Most common: Boston â†’ Portland, Newport â†’ Block Island

**Testing priorities:**
1. âœ… Test coastal passage planning thoroughly (80% of users)
2. âœ… Test offshore passages moderately (15% of users)
3. ğŸ”µ Test ocean crossing minimally (5% of users)

**Features never used:**
- âŒ Don't waste time testing until users request them

---

## ğŸ“Š TESTING METRICS (Revised)

### Old Approach (Pre-Production)
- Target: 90% coverage for all modules
- Measure: Code coverage percentage
- Focus: Write tests before knowing what breaks

### New Approach (Post-Production)
- Target: 100% uptime, <1% error rate
- Measure: Production reliability metrics
- Focus: Fix what breaks, test what users use

**New Success Metrics:**

**Production Health:**
- âœ… Uptime: >99.5%
- âœ… Error rate: <1%
- âœ… Response time p95: <500ms
- âœ… User-reported bugs: <2 per week

**Testing Efficiency:**
- âœ… Regression tests: 100% of fixed bugs
- âœ… Critical path tests: 100% of user flows
- âœ… Feature tests: Based on usage frequency
- âœ… Coverage: Secondary to reliability

---

## ğŸ¯ TESTING PRIORITIES BY WEEK

### Week 5 (This Week)

**Priority 1: Production Fixes**
- Fix orchestrator deployment (P0-1)
- Fix environment variables (P0-3)
- Verify frontend-backend communication (P0-2)
- **Testing:** Smoke tests only (30 minutes)

**Priority 2: Monitoring Setup**
- Add Sentry error tracking (2 hours)
- Add UptimeRobot monitoring (30 minutes)
- Set up basic alerts (1 hour)
- **Testing:** None (monitoring is the test)

**Priority 3: Critical Path Validation**
- Test passage creation manually
- Test weather/tidal data display
- Test safety warnings
- **Testing:** Manual E2E (1 hour)

**Total testing time this week:** 4-5 hours

---

### Week 6-7 (Stabilization)

**Priority 1: Regression Tests**
- Add tests for any bugs found in Week 5
- **Time:** 15-30 min per bug

**Priority 2: Performance Monitoring**
- Add response time tracking
- Set up performance alerts
- **Time:** 2-3 hours

**Priority 3: User Behavior Analysis**
- Analyze usage patterns
- Identify most-used features
- Identify pain points
- **Time:** 2-3 hours analysis

**Priority 4: Targeted Feature Testing**
- Test top 3 most-used features thoroughly
- **Time:** 4-6 hours

**Total testing time weeks 6-7:** 10-15 hours

---

### Week 8+ (Continuous Improvement)

**Priority 1: Ongoing Regression Tests**
- Add test for each new bug
- **Time:** 15-30 min per bug

**Priority 2: Feature Testing**
- Test new features before deployment
- Test based on user feedback
- **Time:** Variable (2-4 hours per feature)

**Priority 3: Performance Optimization**
- Identify slow endpoints
- Optimize and verify improvement
- **Time:** Variable

**Total testing time ongoing:** 5-10 hours per week

---

## ğŸ”„ REVISED TESTING WORKFLOW

### Old Workflow (TDD)
1. Write tests
2. Implement feature
3. Tests pass
4. Deploy
5. Hope it works in production

### New Workflow (Production-Driven)
1. Deploy to production (or fix production issue)
2. Monitor for issues
3. Fix bugs immediately
4. Add regression tests
5. Iterate based on real usage

**Benefits:**
- âœ… Faster time to market
- âœ… Tests for real issues, not imagined ones
- âœ… Better resource allocation
- âœ… User feedback drives priorities

**Trade-offs:**
- âš ï¸ Some bugs reach production (acceptable with good monitoring)
- âš ï¸ Lower coverage initially (acceptable, grows over time)
- âš ï¸ Requires good monitoring (investment required)

---

## ğŸ“‹ TESTING CHECKLIST (Production Reality)

### Before Deploying New Features

**Must Have:**
- [ ] Feature works locally
- [ ] No TypeScript errors
- [ ] No obvious bugs
- [ ] Smoke test passes

**Nice to Have:**
- [ ] Unit tests for complex logic
- [ ] Integration tests for critical paths
- [ ] E2E tests for user workflows

**Don't Need:**
- [ ] ~~90% coverage~~ (coverage is secondary)
- [ ] ~~Tests for every edge case~~ (add as discovered)
- [ ] ~~Perfect test suite~~ (iterative improvement)

### After Deploying New Features

**Must Have:**
- [ ] Monitor for errors (first 30 minutes)
- [ ] Test manually in production
- [ ] Check performance metrics
- [ ] Verify user can complete workflows

**Nice to Have:**
- [ ] Add monitoring alerts
- [ ] Add performance tracking
- [ ] Document known limitations

### After Finding Production Bug

**Must Have:**
- [ ] Fix bug immediately
- [ ] Add regression test
- [ ] Deploy fix
- [ ] Verify fix works

**Nice to Have:**
- [ ] Document root cause
- [ ] Add monitoring to detect similar issues
- [ ] Review related code for similar bugs

---

## ğŸ“ LESSONS FROM PHASE 1 TESTING

### What Worked Well âœ…

**1. Safety-Critical Module Testing**
- Backend testing prevented production bugs
- Navigation calculations validated
- Life-safety logic confirmed
- **Worth the investment:** Absolutely

**2. Systematic Approach**
- Consistent test structure
- Maritime domain expertise applied
- Comprehensive edge case coverage
- **Reusable patterns:** Yes

**3. Documentation**
- Comprehensive test documentation
- Clear success criteria
- Lessons learned captured
- **Future reference:** Valuable

### What Could Improve âš ï¸

**1. Testing vs Shipping Balance**
- Spent 75 hours testing before deploying
- Deployment still had critical issues (orchestrator config)
- **Lesson:** Deploy earlier, iterate faster

**2. Test Execution Environment**
- Tests hung locally (pino-pretty issues)
- TypeScript compilation as validation proxy
- Couldn't generate actual coverage reports
- **Lesson:** Fix test environment first, or use CI/CD

**3. Testing Theoretical vs Actual**
- Wrote NOAA tests before knowing if production needs them
- Spent time on features that might not be used
- **Lesson:** Test based on real usage patterns

### New Approach ğŸ¯

**1. Deploy Earlier**
- Deploy to staging after basic validation (1-2 days)
- Deploy to production after critical path testing (1 week)
- Iterate based on production issues

**2. Test Smarter**
- Fix production bugs immediately
- Add regression tests for fixed bugs
- Test features users actually use
- Don't test features that aren't used

**3. Monitor Everything**
- Error tracking catches issues tests miss
- Performance monitoring reveals bottlenecks
- User analytics show what matters
- Uptime monitoring ensures reliability

---

## ğŸš€ ACTION PLAN

### This Week (Week 5)

**Day 1-2: Fix Critical Production Issues**
- [ ] Fix orchestrator deployment (P0-1)
- [ ] Set environment variables (P0-3)
- [ ] Verify frontend-backend connection (P0-2)
- [ ] Add basic smoke tests (30 min)

**Day 3: Set Up Monitoring**
- [ ] Add Sentry error tracking (2 hours)
- [ ] Add UptimeRobot monitoring (30 min)
- [ ] Configure alerts (1 hour)

**Day 4-5: Stabilize and Monitor**
- [ ] Fix any issues discovered
- [ ] Monitor production metrics
- [ ] Gather initial user feedback
- [ ] Add regression tests for any bugs found

### Next Week (Week 6)

**Focus: Reliability & Performance**
- [ ] Address P1 issues (build time, frontend config)
- [ ] Add performance monitoring
- [ ] Analyze user behavior patterns
- [ ] Test most-used features

### Weeks 7-8

**Focus: Feature Enhancement**
- [ ] Address P2 issues (NOAA tests, health checks)
- [ ] Enhance features based on user feedback
- [ ] Optimize performance based on monitoring
- [ ] Add tests for new features

---

## ğŸ’¡ KEY INSIGHT

**Testing is valuable, but production reality is the ultimate test.**

**Phase 1 taught us:**
- Backend testing was worth it (safety-critical)
- But we could have deployed sooner
- And iterated based on real issues
- Instead of theoretically perfect testing

**New philosophy:**
- Test safety-critical code thoroughly (keep this)
- Deploy other code earlier (change this)
- Add tests based on real issues (new approach)
- Monitor everything (essential for this to work)

---

**Testing strategy now optimized for production reality, not theoretical perfection.**

